text="Okay. I am recording. Okay, thanks. Okay. Um, so welcome. I have to be careful now because I'm being recorded. I have to speak elegantly. Uh, hi everybody. Thanks for being here. My name is Alan Cohen. I'm the head of sales from rev AI. I've been at rev for about six and a half months and this is my third speech company. So I was at uh, <inaudible> and speech Maddix prior to this. So I have pretty vast experience I think in the speech space. I've been here for a number of years and with me chance go ahead.  Hi, I'm Chanse Shifflett. I'm the solutions engineer for rev AI. I come from a QA background so I know revs products very well. I've been with the company for five years.  AI our event. Before we switch over to you guys, I just wanted to let you know that you may see at the bottom of your screen something that says that closed captions are available. So if you click on the little up arrow next to where it says CC, um, you can turn on the live captions and those are done by rev AI. So that is our engine integrated with zone. That looks good. Yeah. I just noticed and we'll see. Okay, great. So it'll, you'll see at least at least two accents. Well, you'll see a Minnesota accent to California accent and an Indian accent. Okay. Here's Surendra just joined us.  Yeah. So you've had multiple accidents.  I know. And 18 and 18 different dialects too. Yeah, I know, I know. That's like one of the most requested, um, tuning things. I've had it actually in my prior companies where people wanted a better indeed support. I'm sorry. Surendra are you with us yet? Okay, welcome. Um, I'm Ellen and with me is chance. I'm the head of sales Chance's solutions engineering. Um, just want to let you know that you can turn on the live captions if you'd like by clicking on the up arrow next to the CC button and those captions are done by rev AI. So you'll get a taste of what we can do in our real time streaming. And I will tell you that our real time streaming is a little bit poorer quality than our async, you know, our post-call and that's because we have more context when we send a file and we have time to actually look at the whole sentence and not just a few words.  So anyway, I'm welcome. AI I'll tell you what I know so far. I know that you guys do conversational analytics and you're looking for an ASR engine to integrate and hopefully partner with to help drive your analytics. And you asked a bunch of questions so we can certainly review those. I think the one that we had the biggest question about was how many channels do you need and what's, what will you do with all those channels? So, um, at that, I'm going to be quiet and I'm going to let you talk and tell us what you need, what you're looking for, and hear any questions you have and then we can circle back.  Sure. Um, thanks a lot. Um, my name is Girish and Kore AI hit the NLP and machine learning team at Kore. Dot. AI. Uh, I've been with <inaudible> for five years now. Um, Kore that is a six year old startup. Uh, we, we, uh, we built the bot platform wherein people can come and build their own bots. So, uh, we have, uh, large enterprises, our customers as our customers from banking, uh, financial <inaudible>. So most of our customers are fortune 500 companies, uh, with huge volumes. So, uh, there are two use cases that we are looking forward to, uh, uh, as a for which we are using Google, uh, is our engine as of now. Uh, so one, uh, since we are in a virtual assistants to business, that is our primary business, uh, we have ASR engines, uh, from, uh, the apps from there. People can talk to the bots and, uh, also from the IVR where people can make a call and a talk that I hear calls the speeches given to ASR engines.  Are those bots, are they, are the IVR, is that menu driven or is it natural language bots? NLU and NLG and all that?  Yes. Yeah. Yeah. So all of them are, uh, natural language words. So, uh, no military stuff. Um, and again, uh, very big, uh, health healthcare companies are in banking companies, uh, using this. And we are using Google as an ASR engine once as an asset engine in some cases.  Are you also using that Google layer for, um, virtual assistance? I can't remember what it's called, but Google has another layer beyond their ASR that, what that shows, like attend to things like that.  Dialogue flow.  Yes. Do you use that as well?  No, that's <inaudible> so that's, that's uh, our main, um, I mean we have, we have our own NLU engines. Uh, so our main competitors as of now, uh, in enterprises are Microsoft Louie. So is a product from Microsoft. And I love flow from Google and IBM Watson. These are the three major competitors for us. Uh, wherever we go for RFPs, um, for the, uh, bigger enterprises. So as of now we are catering mostly to enterprises, not to as amazing, mostly to enterprises. So, uh, um, so that's one area where we are looking, uh, for, uh, alternative way syringes. Then we have another product, um, which is a prebuilt assistant. Uh, we call it as up. So what this, what should assistant will be available on every employee's desktop, uh, as data virtual assistant and doing a lot more stuff than just being a chalkboard and a company Alexa, but enterprises more than Carta.  Yes, exactly. So, uh, yeah, Siri and Alexa for enterprises. Imagine the end raised, uh, ability to, for an employee to talk to an enterprise and XR enterprise, uh, city and, and get their stuff done. Ask information about the company within the company about themselves. How many leads do I have, how many, uh, can I apply for the leave? What is our HR policy and all of that stuff. And it will also help you create meetings, uh, on the fly. I mean, I just can say, uh, Cora, can you create a meeting with Ellen, John's <inaudible> student, uh, for tomorrow and it goes ahead and creates it, uh, for me, uh, ask senior English to information that, uh, we might meet. So, um, this, uh, is already there with a very large beverage company, uh, in, uh, U S and Brazil. We are expanding its features a lot.  Uh, so that it is very, uh, it can be scaled all the enterprises with data place. So one of the expansion plan that we have is, uh, ability to integrate, uh, the ability for this quarter to join the meeting, uh, as a participant. The way your course note taker has joined him, uh, and also, uh, take notes, transcribe, and then, uh, present the notes once the meeting is finished. So now that's something that they would like to also be part of it. So it's, there are two things to, so, one, the ASR on the, um, for the voice assistant where the discussion is a bit too normally two participants, uh, but in the meeting where the, there could be multiple, uh, participants. So, um, I would also like to, uh, understand, uh, whether this course is your implementation or you're partnered with someone else for this. But yeah, so, uh, this thought taker is one of the ideas that we want to have is, uh, as a part of a quota. So that's where you will have, uh, that was the reason why we requested for diarization, uh, ability to identify speakers. And then, um, uh, prepare the transcription so that, uh, anyone can go back and check their, uh, meetings and their transcriptions there.  Right. Got it. Thank you for explaining that. So just to answer your question about chorus, we're not partnered with chorus, we just are paying them, you know, we bought licenses and we're using it and chorus has integrated with zoom and uh, just I'll tell you what I know about chorus and so chorus, this is how chorus used to be that it would appear as a, as a participant in the meeting and then zoom actually did a native integration with them and you just see how it says live on rev live captions appear on the top left. Do you see that? It's at the top, the big red button that says live on rev live captions. Okay. Well, so zoom did an integration with chorus and chorus was actually capturing that third party streaming, um, space. And our captions wouldn't work together with chorus on the machine because of that.  So chorus actually had to back us up to the older version so that they wouldn't capture that stream cause I told them I was going to not use it anymore because I have to have my own captions. So that's why we now have this participant back again cause the participant went away when they were doing live streaming. So, um, you know, it's probably a good idea to keep it as that separate participant because um, other people might want to take the live stream and then you don't have a solution. Okay. So I hear what you, I heard two use cases. One is the bot slash IVR and the second one is the pre-build virtual assistant named Cora, who was meant for employee employee efficiency to help find stuff on the employee network. Book meetings, help great meetings by documents and you're expanding features all the time. You've got, it's deployed, I think you said in one large us company and somewhere in Brazil and um, one of these cases to take notes and then to summarize the meeting, um, much like chorus might do. Um, did I miss anything important there?  Uh, no, I think that's, that's the, those are the two use cases. And uh, definitely the volumes are much heavier in terms of the, uh, IVR bot, bots assistance. Uh, but then we are looking for, because we go for enterprises and now with a lot of meetings going on, we expect the volumes also to be higher even in the corner space.  Can I ask you, um, and are you, are you using, um, Google right now for both of those use cases? Yes. And is there something wrong that's not working for you with Google? Or is this a price thing? What's driving you to look around?  So, um, there are two things that we are, uh, looking for. Uh, one, uh, definitely pricing. And, uh, secondly, we are also looking for on premise solutions because a lot of our customers, uh, on premise because it being a large bank, right?  Banking and healthcare.  Yeah. So they, they prefer, uh, because that's the reason why most of the stuff we built, uh, ourselves we support, uh, on provides. So, um, uh, so yeah, so to cater to them. We are also looking for a solution which can be deployed on premise. Um, that's, that's the second criteria. Third, uh, we are also looking for language support. Um, Google doesn't have a great language support as of now for the ability to customize because that's customization. Um, especially because the domain, uh, domains are different. Uh, hosting four words per domain is, uh, required while putting that,  when you say language support, do you mean English language support or do you mean foreign languages?  Uh, English and foreign languages. So English is yes, available with, uh, not, I mean not just with Google. Uh, I will support English, uh, only as of now. Uh, if I understand the dramatic supports bit more. Um, but then yeah, um, we are, we are already supporting 30 plus languages, uh, for our text-based, uh, assistant. So, uh, there is, there is a demand from, uh, customers asking us to scale even the voice part. Um, so we are also looking for partners. Uh, so we don't want to depend only on one partner. So I don't want to put all my eggs in  eggs in one basket. Right. Um, I have a question. You mentioned that you have tech space as well, so is the idea that you'll transcribe it and then use the same knowledge base for NLU and then, um, the NLG response,  it's happening for the checkbox to the ASR. A voice from IVR goes to Google is from there to the text comes to my analytic. So, uh, we built the entire analytic engine at dark lists, so that's text based. So this text, which is converted by us, our engine is used for intent and entity identification and conversation with the user. Then we returned back text and then we use a text to speech engines to, uh, process that external, uh, uh, give act to the user.  Okay. I'm curious, who are you using for text to speech?  Uh, I don't remember. Aravind Surendra do you remember anything? Do you know, uh, what you're using for extra speech now? I think we don't because I am more, I am more concerned about the NLU engines and conversation engine part. Um, so the PTs part is taken care of by others.  There's a, there's a, there's a company called speech morphing. Um, M, O, R, P, H, I, N. G. Um, and what's unique about them is you can create custom voices that can do natural language with a very few hours of, of sample and you can create whatever voice you want. So like if they wanted to create a, a bot for me so that, you know, it could start taking my calls and only give it to me. If it got stuck, it would sound like me and you don't have to train it on specific sentences, you just train it on a model and then it will be able to say anything in my voice. So it's kind of very cool technology actually. Speech monthly using speech morphing. Yeah. The CEO's name is <inaudible>, F a, T. H. Y. And I think it's foggy at speech morphing if you want, tell them hi if you write down.  So anyway, uh, let, let's talk about what you, what you, so let's leave pricing until the end because the rest has to work in order for pricing to matter. Um, we do have on premise for, for async, for batch, we don't have on premise for real time. So if that's a deal breaker, um, then you should know that, uh, we only have English right now we have all of our English in a single model, which is actually quite good because if you have mixed English, you don't want to have a, you know, a U S model and a UK model and a Australian model because they won't work for the other kinds of people as well. So out of the box you'll get, you know, good English support. Um, so as far as Lang, if you're talking about other language, like, uh, custom terminology for, you know, medical or healthcare or whatever, we don't have a medical dictionary per se, but we have, we have, so I'm not sure you know, the history of rev, but rev has been around about 10 years.  And our core business is, um, human transcription, actually human trips cause transcription and captioning. And as a result, we've got 10 years worth of audio plus human transcription, which is exactly what you need to do in a model. So we have a really vast cross section of, um, of subject matter, you know, legal and medical and all kinds of business and um, and conversational and movies. We've, you know, transcribed a ton of movies. We, at the beginning, we do movies for Amazon before they had their own engine. So we've got, you know, millions of hours that we've to pick from to tune our engine. So you may find that we do quite well. Um, even with your health care clients, um, um, just out of the box, like is your healthcare client that you talked about, are they, um, is it like an EMR, like medical records? Is it more like doctors transcribing or  no, uh, it's again customer support, uh, for the um, medical nurse. Yeah.  Got it. Okay. Um, so it's like mostly so are they doing the call center side of the customer support or are they doing the call classification side? Like agent quality or, or call deflection classification or  it's called deflection. So, uh, the pig around five to 10% of every call, uh, routed to departs as of now, um, which they want to scale the percentage.  Got it. Got it. Okay. Yeah, I've spent a lot of time in, uh, analytics for call center so I know what you're talking about. Um, okay, great. So what questions do you have for us?  Yeah, so a one you said on premise, you don't have, uh, for uh, real time. Uh, it could define, I mean I'm looking, I'm still looking for vendors who can do real name for on device, but as of no years stuff or my transcription work for meetings. Yeah,  let me say it this way. Okay. It's not impossible to do and it actually used to be on our roadmap, but with this whole zoom integration that just got, it took off so strongly that it was pushed back. So, depending on, you know, if, if this is going to be a huge implementation for us and a requirement is to do, you know, real time on prem, you know, if you came and said we're going to give you a million dollars a year, but we require that, that could change our mind about the priority of, of, of, um, on-prem real time cause it was scheduled. Just so you know. So it's not like technically we can't do it. We could.  Hmm. Yeah. I mean, uh, I get it because, uh, now that you are doing some real time, uh, captions, so maybe you could even get into,  yeah, we know how to do it more or less. And, uh, um, again, I'm not going to say we will do it because it became off the roadmap, but again, if there's, if there's big enough business behind it, it can change priorities.  Yeah. I get it. That's how we also work. So for anything, uh, future requested, we see how much can come in as a monetary benefit. And then if yes, then we'll prioritize that. So that's how all of us works. I get that. Uh, yeah, but, uh, to start with, uh, I'm fine with cloud-based, uh, is for a fun port transcription and for real, uh, real, uh, sorry, real time transcription also. Okay.  Can we tell you a little bit about our security because that's probably important. So there's two. There's, okay. So first of all, our philosophy is not to store anything, right? We're not doing analytics. We have no reason to store your data. So there's two ways that you can delete files. Um, one is at the account level, you can set a generic thing for the whole account saying anything that comes through this account, um, deleted after. And there's some choices like an hour, a day, a week, something like that. So you can do that. And then the second one is you can delete by job ID. So basically if you're doing batch, you send your job, you'd get a job AI ID back, you'd get the transcription back, and then you could say, delete job ID number, whatever. Um, so you have two layers of that.  In addition, um, everything is encrypted in transit and at rest. So it's really only un-encrypted very briefly for the transcription. And even if it's a large file, we chunk it so it happens very quickly and then you know, and then again, it goes back encrypted and you can delete it right away. So, and there's no humans involved in any of that. There's no humans poking around. Um, so the only thing that we do store is the fact that there was a transcription and this was the length of it. And that's just for billing. Um, that's it.  So you are, you are entirely dependent on, uh, the, uh, are people like us who would store the data historically in case, uh, for someone else?  Correct. Because we have no reason to store it. Right. There's nothing we're going to do with it. We're not analyzing it. We're not aggregating it. We're not doing anything with it.  Hmm. Okay. Because usually, uh, what, uh, these financial companies also, uh, ask us is okay, what once they, as an exit strategy, if they decide to exit us, they need a proof, uh, that integrity is deleted from us. So we, we take care of that as a,  yeah. I mean, you could put an endpoint on their servers, right, on their S three devices or wherever. And chats, help me out here. Chance.  Yeah, yeah. No, you're right. You can, you can set something up where your story, all of your clients job IDs, right. And, you know, should they decide to exit, you can, you know, hit a button and make sure you may call the delete calls to the jobs. And you know, once, once they're gone on our end there, they're gone. Um, you're not sure  could they store them on their clients servers and not on their own?  W uh, they could store the job IDs, right? If you guys maintain a record of all the jobs itself, the transcriptions. Oh, yes. Yes. So if you guys, and you'll probably want to do that, um, at some point because if you're gonna send the transcription to us and delete it, um, then we're not going to have it anymore, obviously. And so if you guys need some reference or your client needs the file and say they deleted it from their system, um, that would then give them the ability to ask, you know, get rich, retrieve it from you guys.  Well, they definitely need it because, because they're doing analytics on those files, that's their whole, their whole value prop. So if they have a meeting recording for example, and they want to be able to pull out insights from their meeting, they have to have the transcript available. So where are you still, where are you stored as your is up to you basically is what we're saying.  So, uh, you know, once we store it and then if any customer first, uh, decides to exit, uh, if there is an APA, uh, from rev to which I make a call and then I request for deletion of all the cards, because I mentioned the job IDs because I know which customer it is. So, uh, I just need to ensure that all the records, uh, of the side deleted.  So I don't think, I don't think that you're gonna, um, wait until they exit you to delete them. I think you're going to get the transcript back, put it wherever you're going to put it on your system, either on your servers or on their servers, and you're gonna delete it right there. Right. For security, you're not going to want it sitting around. Yeah. Right. Call centers don't want, they don't want data sitting around.  Exactly. Yes. So, especially for a paid, especially for the bad guys who are paying us millions, we don't want their data to be stored elsewhere. Everything has to live with us. And, uh, and even we don't even maintain, uh, logs of, it does everything even for us is encrypted at rest. And even we can't see, uh, unless we have the private key of our customers. So, uh, yeah, that's how, because this handling the financial and health guys is very tough. Uh,  I mean, the job ID doesn't have any identifier, right. Chats. It's just an ID. So we don't know what customer it is. We don't know what's in it. We just know how long it was. Correct. Yeah, that's right. No, we know how many minutes it was and that's it. And that's just so we can bill you.  Sure. Yeah. And uh, coming to, uh, the next one was, yeah. So from the um, meeting thing, and now similar to how this Kore is, note taker is, uh, zoom has this beautiful feature where it can send up to eight to 10 channels if a recording is available. Uh, it compresses it into a video, audio, and then, uh, speaker wise, uh, fight. Uh, but, um, when a bot like this chorus notetaker joins, and usually it records just the voice as if it's a minority you and gives us. So, uh, how does, uh, uh, would you be able to diarize from that? Identify different speakers?  So when we get stuff from zoom, it's flattened. Zoom doesn't give us separate channels, right? So when we record zoom, we're going to get it in a flat file. So, which means we're dependent on software diarization not physical channel separation and chance. If I say anything stupid, please correct me. Okay. So far so good. You're doing great so far. Perfect. Okay. Um, so a software diarization right there, it's, it's dependent on, um, a lot of times it's the quality of the audio. It's how, how similar the voices sound. Someone actually explained this to me once. It has to do with like, it's actually the distance of, of like a body part that goes like your esophagus or something. But that was how it was explained to me by an engineer who worked on it. I'm at a different company, but um, so it's one of the biggest problems of any, um, any speech to text company. I have you encountered that as well? That the diarization is problematic?  No, I think, uh, we have tried, uh, with some, uh, most of these guys are able to do the diarization, uh, from the single, the single audio. Um,  yeah, no, they can all do it. How well do they do it is my question.  Yeah, exactly. So some, uh, I, Google is doing good. Uh, however, the, what I see is their APS are still in beta. So, uh, the public APA doesn't do diarization well, they beta is doing it a really good, um, but they replace a constantly changing, their responsibilities are constantly changing. So that's  so we do do it. Yeah. So we do it in async, we don't do it in real time. Um, we do do it and it's, you know, I've seen calls where we've done it great with four different speakers. Uh, by the way, some companies only they limit it like to two speakers. Um, and then if you have things, so problems I've seen with that, not from this company, from other companies where you could only have two speakers. If for example, there were beeps at the beginning of the call, you know, like it was hold beeps or music or something like that, that would be counted as speaker one. And then both of the speakers would be put on speaker two. Those were problems I've seen. We do multiple speakers, right? So we can differentiate between a beep and music and two different human speakers. Um, and so, you know, we do a fair job edits.  Sometimes we do a great, sometimes we don't. And it really depends on the quality of the audio. I mean, one of the things you're going to see now I think is you're going to see more work at home agents than, than in the past. Right. I've, I've seen a lot of stuff and you know, the, the, the professional publications about that and work at home agents, there's less, um, stringent oversight about things like headsets and how noisy or quiet your environment is. You know, do you have your dog barking and your, your baby crying in the background, all of those things can actually disrupt diarization.  Hmm.  Yeah. So, uh, one quick thing here, uh, about the diarization part. So obviously since you are getting a single audio file, um, you will be able to differentiate between speaker one and speaker two, speaker three, but you'll not be able to tack, um, Ellyn against speaker one man Girish against speaker two or chance against speakers. Because see, uh, from this right,  we don't put, we don't put tag names, right? Chats HSS, speaker one, speaker two, speaker three, right? That's correct.  Yeah. But, uh, usually for enterprises, uh, they need the names of the speakers. So, uh, would there be a possibility where, um, once I tack, because you four, there are two types of customers. One is where customer facing real user facing things where anonymous users could come in and talk to the bot. So I can't put a name to it that I can say speaker one. Um, but in meetings usually there is a closed set of participants. Uh, so, um, if I give the participant list or is there a way you can train, uh, custom models on top of that?  So how would that work? Because this is, you know, you, you have different people in every meeting, every time you have the meeting, right? I mean, are you asking me like, let's say this meeting, okay, there's five of us here. Um, are you expecting us to like to keep a biometric database that we're going to recognize your voice for the next time you were in a meeting?  Uh, yes. So that's, that's again only for, uh, for example, it can be a different slab for enterprises with, uh, with the consent. I can store their biometric and then  yeah, we're not, that's not going to happen. That's not going to happen anytime soon with us. If that's the requirement, I'm happy to refer you to someone who could maybe do that.  Yeah. Okay. So, uh, I'm just, uh, being a bit more greedy. Yeah. Just to understand if that's even possible. I know many of the vendors that currently are not able to do that, that that's a Vish, uh, if someone is able to do it, I would love to take a look at that cause that's a difficult thing to do.  Yeah. I think it really depends on the meeting software you're using. Right. So as we mentioned before, um, there's only so much that's available to us through zoom. Um, so in a perfect world, you know, zoom would be providing us, uh, access to individual speaker channels, um, through the meeting. And then also, uh, providing us with say like, uh, the, the account name of, uh, the speaker on said channel. Right. And then if we, if we had that information and I could definitely see us, you know, building an integration where we're able to, uh, say, okay, this person, any, any noise coming from this speaker in the meeting is going to be chance or any noise coming from this will will be Ellen. Right. Um, but unfortunately that information isn't available to us. And so, um, the speech platform  only has as much information as it's provided,  right? Yeah. Yeah. So I'm coming to pricing. Um, how, how do you price? Is it always, um, I mean, do you slap it in two seconds and privacy?  It's a volume based tiered model just about, just like pretty much everybody else in the industry. So, um, what tier you want me to tell you about? Like how many company hours a year?  Yeah, so as of now, uh, if you see the, uh, the wandering I am seeing is around, uh, uh, around 10,000 calls a day that are coming to my bot, uh, which is coming to around 200,000 calls. But if I can work them in hours, because usually the calls, uh, range from 15 seconds to three minutes. Uh, so if I can word them, it's around $600 that I am getting, uh, there  six, 6,000 hours per what?  Per month is what I'm getting as of now. Um,  Hey, is that just Texas use case number one?  So yeah, let me just quickly calculate 120,000 calls into average of two minutes. Uh, that's what I have as of now. Um, per month.  Okay. Hold on. 20,000 calls times two minutes times two minutes equals well that's, I knew how to do that in my head. Okay. That's full 40,000. Okay. 40,000 minutes divided by 60. Right. That's ours is 666 hours, is that right?  Oh, okay. Yeah. So that's, that's, yeah, 600 hours as of now, uh, with the, I don't know, 10%, five to 10% routing by some guys. So which I am expecting it to scale for the use case one. Um, but for the use case too, um, it's a new idea. So, uh, I don't have any volumes, uh, that I'm having, but then you can add another $200.  Okay. Okay. So, so use case, well, let's just take use case one then. So if I take that 667 hours, times 12, that's hours per year, right? Times 12 months, that's 8,000 hours a year. So our entry level, um, annual commitment is 6,000 hours. So you have passed that. So they um, pay as you go price is $2 and 10 cents an hour and the eight, 6,000 hours or 8,000 hours would be at a dollar 20 an hour. Um, and if you pay, if you do a three year agreement and pay upfront each year, so the payment for the year would then you get a 10% discount. So it would be a dollar eight an hour. So that will come out to with the discount, 6,480 a year or 7,200 without the discount, if that's, if you want to do just one year and a quarterly payments or one year end annual payment, either one. Um, so, and then the next year is 10,000 hours. The next tier is 16,006 67 which is a million minutes. Next tier is 40,000 hours, then a hundred thousand hours and 400,000 hours, one and a half million at 5 million hours. So, and that goes from, from dollar 20 all the way down to 20, 20 and a half sets for the 5 million.  Okay. But when you see hours, yeah, when I get, I get that, but I just want to see the unit of billing. So you'll Google charges 15, 15 seconds, even if it is, say four or five second call detached within seconds. Uh, so how do you, uh, your, your slab is also a great 15 seconds or  so. Normally it's 15 seconds if you get up into the higher tiers. Um, I would be willing to go to bat for you with our engineering to remove that restriction for you. Like, I'm not gonna do that for 6,000 hours. It's just they, they'll, you know, they'll get mad at me for wasting their time. But if you, but if you know, for up in the higher tiers, you know, 400,000 and above, I think we could probably get that eliminated and charge you by the second. Cause I know that it calls center that's really meaningful, especially for things like voice bots where you know, some of the utterances are five seconds and you're ended up paying three times that with Google. I know the problem. Well.  Yeah. Yeah. So, um, again, that was my next point where when you're doing the conversation, uh, with the Barts, um, I am sending, uh, API requests. So that audio stream could be just five seconds or 10 seconds. So will it be convert it to 15 seconds or will you take the entire session and a number of times I have streamed to you and the total size of that, uh, audio  chance. Do you know the answer to that? Um, for streaming, no. Cause they're going to, so think of, think of a, uh, a voice bot answering a phone when you call your bank. Okay. And they say, how can I help you? You say, I want to make a deposit. Okay, you just talked for three seconds. Okay. And then they come back and the voice by says, okay, I can help you with that. And I think that what's your account number? And you're going to give them a number that's going to be another three seconds. So he's asking, is this going to be three seconds plus three seconds plus three seconds and it's all going to add up to 20 seconds? Or is each one of those going to be built at 15 seconds each three seconds segment? I would say that it depends on um, on how you set up the streaming, right? If you are setting it up so that you're establishing a new connection to the stream every single time, uh, someone  speaks, uh, that means that you'd be charged 15 seconds for each stream. Right? Uh, whereas if you open the stream and then, uh, keep it open for the entirety of the call, you will be charged for the entirety of that call.  Okay. So in other words, each time you open and close the stream, the amount of time of that stream from start to finish is calculated. And if you have the minimum segment thing of 15 seconds, if it's less than 15 seconds, you'll be charged for 15 seconds. If you leave the stream open for the whole thing, um, it might actually come out cheaper.  Yeah. But usually if I keep it stream open, it's like a synchronous. So, uh, I don't think, uh, it is a good viable solution even for rev.  If you, if you keep the stream open, what you're getting back is these Jason's snippets that are being streamed back to you via a web socket as they come through.  Yeah, I agree. I agree. But technically what I'm seeing is that say I have 5,000 calls or even let's say a hundred parallel calls under a hundred concurrent calls, I have to keep a hundred connections open with rep and tomorrow if it increases that many connections will increase. So,  right. So then it's going to be, yeah. Okay. Um, you know, I, it seems to me like the, you know, once you launched this, um, you know, the call center thing, you're going to be in volumes that, you know, it would be worth it for us to, to stretch to make that happen for you and get rid of that 15 second minimum. Yeah. I mean it's not, it's, it's not, that's not a technical impossibility. Right. That's something that if the deal justifies it, I'll get rid of those 15 second minimums.  Yeah. So it would be great if what we can do with the way a billing could be that you just aggregate the total number of seconds that, uh, I made calls to you over the month and then invoice on that instead of,  yeah, I hear, I hear that. That's what you want. I mean it does take a little bit of work to do and we have done it in the past for very large customers. You know, cause there's only particular use cases that that really matters for, you know, if someone's transcribing, you know, movie transcripts and they don't care because they don't do anything that's 15 seconds or less. But call center does. It does, it does matter. I get it. I get it. Believe me, I get it. Um, I used to actually, that's how I used to sell against Google is I was used to raise that when I was working in bocce. I used to say as an advantage. Um, so I'm going to just make a note in my notes that that's important to you. Okay. Okay.  Okay. I think that's all I had. Um, and then yeah, I think my team is, uh, Aravind uh, Aravind heads, uh, my R and D department, um, uh, student, uh, is, uh, my architect in, uh, I mean he built the entire knowledge ontology engine, uh, at code RTI. He hits that team of, uh, natural language processing machine learning.  How long have you guys around? Did you say? Six years. I think I missed that at the beginning.  Yeah. The company is six year old. I'm in the company for the last five years. Uh, I'm, uh, I'm in the it industry for almost 20 years now  and I was just taking notes after my, so can you tell, so Girish is doing the, um, the NLP, right?  Yeah, I heard the, I had the interior and LP and machine learning and NLU part, the conversational intelligence part is something which I had in the company and uh, Aravind and Surendra work with me and uh, Aravind is responsible for doing these initiatives, research and development on, uh, uh, the meetings. And there's a lot of this part, unfortunately he had to drop off because he lost internet. Um, so, um,  can, can you guys tell me the, like the, so first of all the decision making process, like is it the three of you that's going to decide together what you do and then also the timing. Like how urgent is this? Is this something you really want to do to change from Google or is it kind of working, but you're just seeing if there's better things out there or, um, you know, it's, give me, let me understand your, the business side of this.  See, uh, what I'm, uh, responsible for is definitely the technical evaluation. I'm talking to multiple vendors because I want to save cost from Google because as the volumes are increasing, my bill for Google is increasing. So I want to see an alternative for that. Uh, definitely place. Uh, the rebates are constantly changing, so I want to look for someone, uh, who's a bit more stable. Uh,  does it, does it also matter that like essentially you are funding the competition by, um, by, you know, they, their dialogue flow is your competitor and you're giving them money.  Uh, that's, that's one of the, uh, issues also because when we go to RFPs, um, I can't tell this officially, but when I go to RFPs, I am competing against them for ASR. I'm using them. So, yeah. Yeah. Yeah. So, um, I that the reason is, again, Google is not on prem. Um, so, um, that's, that will be one of the biggest deal breaker for me in the longterm where if I can partner and we have partnered with some of the other, uh, companies for some other things where we are the single, uh, vendor for, uh, our enterprise and uh, because they don't have to talk to multiple vendors now they don't have to pay Google and they don't have to pay us. So, um, we have the single vendor and then uh, we uh, we pay to our vendors from whom we are taking the services. Right. So,  well how, what platform do you host? Are you on AWS or Google or Microsoft or some other thing?  We are by default, our cloud is on AWS.  Okay. So our cloud is on AWS also, which means you would also save, you know, anger is fees or whatever it's called, you know, transferring of data cause it's, you're transferring it into AWS. Um, and so our on prem, we should just tell you our, our on prem as a Docker container that you can install anywhere you want to install it. I just want to point out, it does not come with the orchestration layer of, so we chunk files. When we get an hour long file, we chunk it into five minute chunks and um, so it gets done in parallel. And so pretty much any amount of audio you send us will get done in like less than 15 minutes because it's all chunked and distributed. And we've got, you know, hundreds of machines running, cause we've got, you know, 170,000 customers running on rev. So, um, you know, the performance is very consistent. I would say. Um, the only problem is, is that if you do on prem, you have to do your own Chucky.  Yeah. I think that should not be a problem. Uh, yeah. But, uh, as of now, I have, uh, I have not, uh, received, uh, inquiry where customers are forcing, forcing us to be on prem. Uh, but definitely the, once I launched the meetings, uh, definitely they would like to be on prem because they wouldn't like the meeting recordings to be sold, uh, in cloud, the banner banking and financial.  So remember, you can be, just remember you can have a transcribe in a cloud but not have it stored in the cloud.  Yeah. That's one of the solutions that we are proposing it to them saying that only for transcription we'll use cloud, but the storage will be on that. Yeah,  yeah. We're working on, yeah, I imagine we're going to have HIPPA and SOC too. You know, I can't tell you by when we just got a new security guy like last week, but, um, it's our intent to just, okay.  Uh, Hey Ellen, I have a question in mind.  Yes.  So, uh, the vendors that we were evaluating, so they said they have different models for different elegant, uh, I just missed that part. You said like you have a single model, uh, uh, companies have, uh, data from different dialects, so, uh, so users then in done it'll perform better across dialect. So have I heard it right?  Yeah. So I'll explain what I meant. So let's say you have an American, a Brit, an Indian, a German, and a French person all in the same meeting. Okay. And let's say you're with a company that's got a U S English model, UK English model and Australian English model and Indian English model, and they don't have any model for German or for whatever. You don't switch models. I mean it's, it's a time consuming thing to switch models. You have to switch them in and out of memory. You takes a lot of Ram. You know, you don't, you can't load like four different models. It's just there. Usually it's not enough memory to do that. Um, so what are you going to load? You know, and, and you can't start, you know, manipulating which model you're loading every single time you start a meeting, right? I set up a meeting, I put it in my calendar, I put it in zoom and boom, it's ready to go.  Nobody's asking me who's in the meeting and what kind of English they speak, and I don't even know what kind of English they speak. You know, you guys invited three different people. I could guess from your names that you might have an Indian accent, but maybe not. Um, you know, I didn't know what country you're living in actually. Um, and so, um, that's why I say across the board, we tune our models and put all of our dialect support into a single model. So across the board with mixed speakers, um, you will have better results from us as opposed to a company that says, okay, if somebody's British and they're being transcribed with the U S model, it's not going to be as good.  Okay. Got you.  Like, I could tell you that vote, she in their us English model, they tuned with, you know, a Louisiana accent and a Boston accent, Minnesota accent, whatever, but they didn't have British, you know, that's not to say it couldn't do anything with the person speaking British, but it wasn't, it wasn't tuned to transcribe people speaking British English.  Uh, okay. Thanks for answering that. I have one more question here. Um, so I have also seen as part of, uh, as part of our, uh, as part of this platform, I could, um, I could read my customer vocabulary. Uh, so is there, is there any way where I could, uh, highlight the importance of certain words against other words? For example?  Yeah, we have a custom, yeah, we have custom vocab. Uh, we actually have some improvements coming out at it and like next week I think. So you can list your own words there. And it's oftentimes people's names, product names, internal product names, um, names of cities maybe that are obscure that we don't know about or, um, but yeah, you could put your own custom vocabulary words in there as well. And that's per API call so you can actually maintain a may list chance. Do you want to talk about that at all?  Okay.  Um, sure. But I think, uh, did you want to finish your, your, your, your question? It sounded like you, you had a little more to  uh, right. So I saw they did a custom vocabulary support, but then some words are more important than other words. So is there any way I could, so within the list of custom vocabulary, I could highlight the importance of certain words or other words.  What makes a word more important?  Yeah. So, so for example, uh, let's say, uh, our company name is Kore dot. AI, but, uh, most EHRs, um, most, uh, speech to text engine will transcript Kore <inaudible> against <inaudible>.  Okay, got it. Got it. Okay. So first of all, for us, there are no words that are more important than other words. In other words, we try to get every word right. You know, our goal is to transcribe every single word. Right now we don't always do that obviously, but we try. But we are implementing a feature and that's one of the improvements that's coming. We call it word boosting. So if we have CR E in our dictionary and you put KRE in your custom vocabulary, word boosting would cause us to take your word instead of our word. And so basically every word that's in your custom vocab that conflicts with one of our internal words would be boosted to be more important than our words.  Okay. Okay. So when is this expected  chance? Do you know when we're expecting that?  Um, uh, I don't know about the word boosting. Uh, I'm not sure when, when we're going to be releasing it. We have recently just, uh, we're, we're releasing a new update, um, that deals with things like true casing and, um, and uh, other things. Um, but it's definitely, I would say on our roadmap, we hope to have it out  near term. It's on our near term roadmap. So we do agile development and I think you guys probably know how that goes. It's like sometimes it doesn't make it, it gets pushed to the next sprint, you know, depending on test results, et cetera. So it's supposed to be, I'm not sure if it's the one coming out next week, but I think it will, I'm pretty sure we'll see it within, like, if I had to guess, I'd say we'd see it within like a month or month and a half at the latest.  Would you agree with quickly, do you do the production realism?  Yeah, I'd agree with that statement. How, um, um, as far as the production releases, um, with it, it varies. Uh, we have multiple teams. Um, but if, if you're, uh, talking about the language model updates specifically, um, those usually come out when, when they're ready. Right. Um, they don't have a regular cadence. Um, and so they require a lot more testing. Um, and we also like to run it internally to make sure that, you know, we're, we're not messing anything up. Uh, so it, uh, we play chances at four times a year is a twice a year. It, it varies. I can't, I, I can't say either. It depends on the changes that are coming out and, uh, what can, what can fit into it. So I'm not sure we all understand what type of release you're talking about. How about like, just regular new features, bug fixes? They'll seem to come out like every, is it two or three weeks or something? So for, for API related stuff, um, it's, we have a weekly release, so when we see bugs, um, or, or, or, or more rolling out new features, there's a weekly release that happens. But I was specific, I was talking specifically about the language model updates that we do. Mmm. But everything else is weekly.  Okay. So coming back to this, uh, custom vocabulary, uh, I understand the evens defend Surendra tried, uh, uh, you know, in APA you allow us to propose custom vocabulary. But, um, the challenge there sometimes is, um, if it's a, say a meeting recording or a movie or something, uh, I make one APA call pass you the, uh, audio file and give you the customer cap effect. Makes perfect sense. Uh, but, uh, in the really cons, uh, with customers where the number of AP calls could be more, uh, passing. So much of customer vocab for each API call, uh, could, uh, unnecessarily increase the, uh, API size. So would there be a way with, uh, if a request is from, um, again, the cost or customer coverage changes from, uh, different customers for us? Because one could be a banking, uh, and within banking one could be a retail banking, one could be, uh, risk, uh, banking. Another could be a market. So, uh, different, different stuff. So let's healthcare. So is there a way, uh, we can, uh, tell you that it requests from Kore plus request from Swan soca accounted your customer ID? Uh, you store this customer cabinetry with, uh, and then for every request I just give you the customer account ID or whatever, and then you pick the vocab and then do it unless I send you an updated vocabulary.  Yeah. So if you're talking about for streaming, um, then yes. That that is the way custom vocabulary works with streaming. Um, what would you say?  You said that we would store their custom vocabulary and every time, every time that account Mmm. Run something we would access their lists. You're saying that's the way it works?  No, so what I'm saying is that, um, what they will do is they will send a custom vocabulary job and uh, provide us with a custom vocabulary. We will return them a job ID that they can then store and associate it with whichever customer it would belong to. And then every time an API call is made, uh, you with a stream from said customer, they can attach that vocabulary ID to the, the URL as a parameter. And we would use that custom vocabulary for that, that, uh, real time stream.  So if they have a call center with a hundred agents, okay. For bank ABC, you send it every time. If they've uploaded at one time with the vocabulary for bank ABC, any one of those hundred agents that's making calls, wood, their API calls would access the same custom vocabulary,  uh, as long as they included that custom vocabulary ID with the streaming call.  So this, the, the customer vocabularies, they work differently on async than they do on streaming, or at least I'm passing them in with the job is different. Um, async, you're required to pass in the entire vocabulary, right? So it's all the, the entire list of phrases. Um, but with streaming, what you need to do is you need to create a custom vocabulary job. And, uh, once that job is done processing, you can then use that custom vocabulary ID with a stream and attach it. And we will, we will use that custom vocabulary, um, with that stream. And you can do this with a number of customers, right? So if you have, uh, you know, say a, um, a medical call center, right? Uh, or a medical customer, um, and you need, um, medical vocabulary. You can create a custom vocabulary job, um, for that customer store, that ID on your guys's end. And then whenever calls come in, uh, for that customer, you would just attach that custom vocabulary ID, uh, as a parameter to the streaming URL. And we, we use that custom vocabulary with that stream.  I think that would be great. Yeah, that's something because I don't want to send duplicate content.  Right. And so you, I think the most important thing is just maintaining that ID and associating it with that customer on your side. And as long as you guys do that, be a problem. Okay.  Yeah. So we do that always because a natural mother. Yeah. So then are you asking us, it's a chance, uh, this customer will get the ID with this feature also be available for, can be made available for a sync request because I heard that it's only available for streaming requests.  Uh, so what you heard is correct. Yes. It's only available for streaming. Um, at this time, uh, as Ellen mentioned earlier, you know, um, are our focus right now has been captions and so we had a bunch of other things, you know, uh, on the roadmap and so actions is taken a very high priority. Um, so I can't say that it's, it's not going to come to async, but as it is right now, um, the, that's not the way it works. You'll have to pass in those custom vocabularies with every single job for asynchronous jobs.  AI let me, I'll say something else about that. You know, that the volumes that you were talking about, if you're doing, you know, large volumes with huge call centers, um, for us that would be a significant customer. Okay. For Google, that probably isn't a significant customer. So it's better to be a significant customer to a company like ours than, um, to be an insignificant comfort customer because you'll have more influence on the roadmap, you know, and I think that our priorities are also going to revert back to what they were when covert 19 is behind us and we can have some normalcy back again. So, um, what I want to say is, you know, our engineering team are there, pretty much most of them are from MIT. Our founders are all from MIT, they were all friends at MIT, really smart people. It's, it's not really if it can be done because pretty much anything can be done and there's a very can do. Yeah, we could do that. And a lot of research and that's just the nature of our engineering team. It's just a question. But we're also very focused on our priorities and we, we like, we're like a ship, you know, we March in the same direction. We sail towards the same Island. So, um, if this becomes important to a big enough customer, things will get on the roadmap.  Sure. Um, how big is the engineering team? At REM  do you know, chats, we've got a bunch of different teams are going to say we're 150 people. Um, I would say that the engineering is probably like between 50 and 70. I don't know, between here and Austin. What would you say? Chance. I want to say it's the bigger half of the organization we have. We've always been pretty engineering heavy. Um, right. So, but I, but I, I can't, I can't come up with a number off the top of my head. We've got, we've got the language team, we've got the data science team, we've got the, the, you know, the front end team. Cause you know, some of our products have go, he's on the other side of the business. We've got the, you know, the API team. So there's a bunch of different teams that uh, they add up.  Okay. And, um, when you find me, um, well who are the, can I get some references of the customers? Uh,  we'd have to make sure that they're okay being referenced customers. Now you have to remember that rev AI had launched about two years ago to external. We've been using an interlink for a number of years. So many of our customers are new. So I'm the first sales person that was actually hired and I've been here six months. Okay. So, um, you know, it's not like I can give you someone who's been using this for, well actually some probably have been using it for two years. Um, I would need to find out if they're willing to be referenced customers. So I'd like to save that until court, you know, closer to the ad. Like if everything else lines up for you. Um, certainly I can, uh, I can tell you this. Okay. When I, my first month on the job, I was, I came to San Francisco for a week for training and then I went to conferences and, um, I was at a conference and I'm trying to remember which conversation it was and I think it was NAB, New York and I had people coming up to me all day long saying, I just wanted to stop by and say, hi, we're rev customer.  We love you guys. You save us so much time. And I was hearing that again and again, I've never, I've been in sales 10 years and I've been in other things like 25 years. I've never had that experience before. It was really kind of fun.  Okay. Okay. Uh, I think, uh, if we cross 40 minutes, uh, thanks a lot for your time guys. Um, uh, just wanted to check, I mean I think my team has, uh, created some accountant. They are trying, but uh, would there be a way you can help us? Uh, providing the paperwork and just for, uh, repairing some benchmark, um, with some sample audios because we have been tracking with somebody.  Did you ask us to provide you with the API token? Yeah. So you can do that yourself. It's all self serve. You just go to your account and you just go get token and you'll get a token.  Oh, okay. So that doesn't have any limits on the cloud is it?  So you get five free hours, um, if you need more time I can add you some more. Um, you know, within reason. So  that should be fine. I think for us should be fine. For me. So in case I need something, I'll come back to you tomorrow.  Okay. When should we check in with you again?  I love, um, come back to you on Monday. Okay,  that's awesome. Yeah, I'm in the truth. The truth is we've seen like complete end implementations. Um, and literally days like I talked to one customer, they were using something on the other side of our business and we raised the price over there and he didn't like that. So he wanted to move over to us and he was telling me he doesn't have a developer and he doesn't know what to do. And I said, well, you know, just go to Upwork and to hire someone for two days. That was on Tuesday. Friday he had a working implementation. So, um, yeah,  so how, yeah, how I want to do is I finish up my exercise by Friday and then, uh, I present something to my CEO and CTO on Monday. And, uh, maybe I reach out to you guys. So if everything goes well, uh, I want to set up a sandbox environment. Um, we already have a sandbox where, um, any calls made to demo number is currently going to go, Lisa. So I want to deflect some calls to drive, uh, any other vendor whom I feel is good. And then, um, prepare a report from that sandbox, uh, as to how Google is doing, how this is. And uh, then also I can show the cost analysis. Uh,  what are they recording with, by the way, for this customer you're talking about?  Sorry. No.  Who is, what recording are they using?  They don't take it. They just make a phone call on this number. We tell that our bot can search queries on these topics and for demo purpose they will make a call directly and they'll talk to the bot. Uh,  how are you recording the call? Is my question, what are they using? What tool are they using? What are they using? Asterisks? Are they using nice, are they using five, nine?  What do you mean it's a streaming, since it's not recording, it's a stream. So Paul's go to Twilio. Yeah, yeah, yeah, correct. Twilio. So it comes to Twilio and from Twilio to an ASR engine problem. Yep. So, um, Aravind do you know if, uh, from Twilio we can make any rebook call right? To now we don't have restricted to good. Yes. We can make, uh, level calls to any service. That's how it is. So, so this online account that we created, uh, Ellen and chance that I can use it even for streaming, right. It's not just a sink.  Correct. And the pricing, the pricing I gave you was the same for both.  Okay. Great. So I want to set up this and a demo it because now I um, it's usual same problem that we face, uh, with our enterprises. Uh, even after, uh, Google is infamous for uh, privacy issues. Still people trust who will come back to, uh, other startups, right? So, uh, similarly I need to now prove that you don't have to worry about this so you can try it.  We are, we are, we are happy. We are happy to either meet with or answer any sort of security press questionnaires. We've got a zillion security policies that we can attack. So we do that with every customer. So we're very used to it. We've got a guy that that's all he does.  Thanks a lot. Uh, I, uh, update you on Monday. Uh, what the next steps, uh, what we can do, uh, how we want to proceed and then if required, we can have another  because we should next week. Sounds great chance. Can you hang on for a second when we're done? Okay. Thank you guys very much for your time and for considering us. Okay. Bye bye. Thank you. Bye. Bye. Okay."
